{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee02879",
   "metadata": {},
   "source": [
    "Building A Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c2519c",
   "metadata": {},
   "source": [
    "In these video. We will go over an example of how to design and LLM powered chatbots.This chatbot will be able to have a conservation and remeber previous interaction.\n",
    "\n",
    "Note that this chatbot that we build will only use Language model to have a conservation. There are several other related concepts that you may be looking for :\n",
    "\n",
    ". Conversational RAG: Enable a chatbot experience over an external source of data.\n",
    ". Agents: Build a chatbot that can take actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "227b7fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() ## loading all env variables\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8492943c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001D3D8753700>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001D3D87512A0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model = \"gemma2-9b-it\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37ddf9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Naidu, it's nice to meet you!\\n\\nThat's an impressive title! As a chief AI engineer, I'm sure you're involved in some exciting projects. \\n\\nWhat kind of work are you currently focused on?  I'd love to hear more about your role and the challenges and opportunities you're facing in the field of AI.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 23, 'total_tokens': 102, 'completion_time': 0.143636364, 'prompt_time': 0.002138888, 'queue_time': 0.06620019099999999, 'total_time': 0.145775252}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--3ad9392d-4b06-4b79-bac7-6cba1b17af50-0', usage_metadata={'input_tokens': 23, 'output_tokens': 79, 'total_tokens': 102})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([\n",
    "    HumanMessage(content=\"Hi,my name is naidu i am a chief AI enginner\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7456e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"It seems like you're asking what you, as Naidu, the Chief AI Engineer, will do. That's a great question! \\n\\nAs a Chief AI Engineer, you'd likely be involved in a variety of exciting tasks, such as:\\n\\n* **Leading the development and implementation of AI solutions:** This could involve anything from building machine learning models to designing and deploying AI-powered applications.\\n* **Researching and staying up-to-date on the latest AI advancements:** The field of AI is constantly evolving, so it's essential to be aware of the newest technologies and trends.\\n* **Building and managing a team of AI engineers:** You'd be responsible for hiring, training, and mentoring a team of talented individuals.\\n* **Collaborating with other departments:**  AI can have a wide range of applications across different industries. You'd likely work with teams from various departments to identify opportunities to leverage AI.\\n* **Ensuring the ethical and responsible use of AI:**  As AI becomes increasingly powerful, it's important to consider the potential impact on society and take steps to mitigate any risks.\\n\\n\\nWhat are some specific areas of AI that interest you, Naidu?  Perhaps we can brainstorm some ideas for projects you might undertake! \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 263, 'prompt_tokens': 98, 'total_tokens': 361, 'completion_time': 0.478181818, 'prompt_time': 0.004237896, 'queue_time': 0.084471804, 'total_time': 0.482419714}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--4d2a13ca-b050-4928-bead-c54ea2fb23cb-0', usage_metadata={'input_tokens': 98, 'output_tokens': 263, 'total_tokens': 361})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "         HumanMessage(content=\"Hi,my name is naidu i am a chief AI enginner\"),\n",
    "         AIMessage(content=\"Hello Naidu, it's nice to meet you!\\n\\nBeing a Chief AI Engineer is a fascinating role. What kind of projects are you currently working on?  \\n\\nI'm always eager to learn more about the innovative things people are doing with AI.\\n\"),\n",
    "         HumanMessage(content=\"Hey what it's my name what I will do ?\")\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e5a60",
   "metadata": {},
   "source": [
    "## Message History\n",
    "\n",
    "We can use a Message History class to wrap our model and make its stateful.This will track of input and outputs of the model,and store them in a datastore.Future interactions will then load those messages and pass them into chain as a part of input .Let's see these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a7c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "def get_session_history(session_id:str)-> BaseChatMessageHistory:\n",
    " \n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d4d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"configurable\": {\"session_id\":\"chat1\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83788653",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(\n",
    "        content = \"Hi,my name is naidu i am a chief AI enginner\")],\n",
    "config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b0e847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(\n",
    "        content = \"Hi,my name is naidu i am a chief AI enginner\")],\n",
    "config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bd07831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Naidu! It's great to meet you. \\n\\nBeing a Chief AI Engineer is a fascinating role. What are some of the most challenging and rewarding aspects of your work? \\n\\nI'm curious to hear about your experiences and the kind of projects you're involved in.  \\n\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e7014fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config2={\n",
    "    \"configurable\": {\"session_id\":\"chat2\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd1d2719",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"my name is john\")\n",
    "    ],\n",
    "    config=config2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ca94555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello John! üëã\\n\\nIt's nice to meet you. \\n\\nIs there anything I can help you with today? üòä\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8a043fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is John!  üòä \\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### with diffeent session_id\n",
    "\n",
    "\n",
    "\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(\n",
    "        content = \"what's my name\")],\n",
    "config=config2\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f80a2",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "Prompt Template helps to turn raw user input into format of LLM can work with.In this case the raw input is a message,which we can passing to llm,First add system message with some custom instruction (but still take message as input),next we will add more input beside mesage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ec57d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "pt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant i need your help for my question of your best ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = pt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce24285b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Krish! üëã \\n\\nI'm happy to help. What can I do for you today? üòä  \\n\\nPlease ask away, and I'll do my best to provide a helpful answer!  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 32, 'total_tokens': 78, 'completion_time': 0.083636364, 'prompt_time': 0.002298138, 'queue_time': 0.083066268, 'total_time': 0.085934502}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--273bbd63-b261-440b-be08-d0c76864738f-0', usage_metadata={'input_tokens': 32, 'output_tokens': 46, 'total_tokens': 78})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi, My name is krish\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8839e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history= RunnableWithMessageHistory(model,get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "063c1edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi,My name is krish\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07f14654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Krish, it's nice to meet you! \\n\\nWhat can I do for you today? üòä  \\n\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11f8a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "pt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your {language} ability.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "chain = pt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "676063aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§®‡§æ‡§Ø‡§°‡•Ç!  ‡§Æ‡•Å‡§ù‡•á ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•à ‡§§‡•Å‡§Æ‡§∏‡•á ‡§Æ‡§ø‡§≤‡§®‡•á ‡§Æ‡•á‡§Ç‡•§  \n",
      "\n",
      "(Namaste Naidu! It's nice to meet you.)\n",
      "\n",
      "‡§ï‡•ç‡§Ø‡§æ ‡§Æ‡•à‡§Ç ‡§§‡•Å‡§Æ‡•ç‡§π‡•á‡§Ç ‡§ï‡•Å‡§õ ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Å?\n",
      "\n",
      "(Can I help you with something?) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"language\": \"Hindi\",\n",
    "    \"messages\": [HumanMessage(content=\"Hi, my name is Naidu\")]\n",
    "})\n",
    "\n",
    "print(response.content)  # ‚úÖ OR just: print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592b632",
   "metadata": {},
   "source": [
    "### Let's now wrap this more complicated chain in a Message Chat History class.This time ,it has multiple key in the input.we need to give a correct input to save the histyory chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12030b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history= RunnableWithMessageHistory(chain,get_session_history,input_messages_key=\"messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae361830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§®‡§æ‡§á‡§°‡•Ç! \\n\\n‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡§π‡§æ‡§Å ‡§π‡•Ç‡§Å‡•§ ‡§Ü‡§™ ‡§ï‡•ç‡§Ø‡§æ ‡§ú‡§æ‡§®‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç? \\n(Namaste Naidu!\\n\\nI am here to help you. What would you like to know?) \\n\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 32, 'total_tokens': 88, 'completion_time': 0.101818182, 'prompt_time': 0.002291098, 'queue_time': 0.065698704, 'total_time': 0.10410928}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--6fc8487b-cd56-4936-957c-60be436ab0ad-0' usage_metadata={'input_tokens': 32, 'output_tokens': 56, 'total_tokens': 88}\n"
     ]
    }
   ],
   "source": [
    "config={\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Hi, my name is Naidu\")],\n",
    "        \"language\": \"Hindi\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13a767ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§®‡§æ‡§á‡§°‡•Ç ‡§π‡•à‡•§ (Your name is Naidu.)  \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response =with_message_history.invoke({\n",
    "    \"messages\":[HumanMessage(content=\"what's is my name\")],\"language\":\"Hindi\",\n",
    "},\n",
    "config=config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "382224ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§®‡§æ‡§á‡§°‡•Ç ‡§π‡•à‡•§  üòä (Your name is Naidu.) \\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response =with_message_history.invoke({\n",
    "    \"messages\":[HumanMessage(content=\"what's is my name\")],\"language\":\"Hindi\",\n",
    "},\n",
    "config=config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5111581",
   "metadata": {},
   "source": [
    "### manage the conversation history\n",
    "'''\n",
    "one important concept to undersatnd when building chatbots is how to manage the conversation history.If left managed,the list of messages will grow unbounded potentially overflow the context window of LLM.Therefore it is important to add a step that limits the size of messages you passsing.\n",
    "\n",
    "'''\n",
    "\n",
    "\"trim_message\" helper to reduce how many messages we are sending to the model.The trimmer allow us  to specify how many token we want to keep,along with other parameters like if you want to keep output paramters like if we want to always keep the system message and wether to allow partial messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "479261c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are a good assistant\n",
      "I like vanilla ice cream\n",
      "nice\n",
      "what's 2+2\n",
      "4\n",
      "thanks\n",
      "no problem\n",
      "having fun?\n",
      "yes!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain_core.messages import trim_messages, SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# ‚úÖ Messages list\n",
    "messages = [\n",
    "    SystemMessage(content=\"you are a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm Bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"what's 2+2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "# ‚úÖ Trim chat history based on token count\n",
    "trimmed_messages = trim_messages(\n",
    "    messages=messages,\n",
    "    max_tokens=45,\n",
    "    token_counter=model,          # model with get_num_tokens_from_messages()\n",
    "    strategy=\"last\",\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "\n",
    "for msg in trimmed_messages:\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2ab4a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked \"What\\'s 2+2\". \\n\\n\\n\\nLet me know if you want to try another one! üòä\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Define a function to trim messages dynamically\n",
    "def trim_input_messages(inputs):\n",
    "    # Use the same trimming logic as before\n",
    "    return trim_messages(\n",
    "        messages=inputs[\"messages\"],\n",
    "        max_tokens=45,\n",
    "        token_counter=model,\n",
    "        strategy=\"last\",\n",
    "        include_system=True,\n",
    "        allow_partial=False,\n",
    "        start_on=\"human\"\n",
    "    )\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=trim_input_messages)\n",
    "    | pt | model\n",
    ")\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"What Math Problem did I ask\")],\n",
    "        \"language\": \"English\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad7c629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets wraps in a message history\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\":{\"session_id\":\"chat5\"}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a787d8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked \"What\\'s 2+2\".  \\n\\n\\n\\nLet me know if you want to try another one! üòä \\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response =with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"What Math Problem did I ask\")],\n",
    "        \"language\": \"English\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec61c8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a large language model, I don't have access to past conversations or any personal information about you, including your name. \\n\\nWould you like to tell me your name? üòä\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response =with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"What's my name\")],\n",
    "        \"language\": \"English\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
